# -*- coding: utf-8 -*-
"""document_similarity - NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E0MnZmH7GV6CpA5sQhzqTcIN-LYwdcve
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import numpy as np
# import nltk
# nltk.download('punkt')
# from nltk.corpus import wordnet as wn
# import pandas as pd
# from sklearn.metrics import accuracy_score
# 
# nltk.data.path.append("assets/")
# 
# from nltk.tokenize import word_tokenize
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# from sklearn.feature_extraction.text import TfidfVectorizer
# from nltk.corpus import wordnet
# nltk.download('wordnet')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('omw-1.4')
# nltk.download('stopwords')
#

"""The goal is to compute the symmetrical path similarity between two documents. To achieve this, you need to complete the following functions:

1.   **doc_to_synsets:** This function takes a document as input and returns a list of synsets present in the document. First, the document is tokenized and part-of-speech tagged using nltk.word_tokenize and nltk.pos_tag. Then, for each token, the corresponding synset is found using wordnet.synsets(token, wordnet_tag). The first synset match for each token should be used, and tokens without any synset match are skipped.

2.   **similarity_score:** This function calculates the normalized similarity score between two lists of synsets (s1 and s2). For each synset in s1, the synset in s2 with the largest similarity value is found. The largest similarity values are summed, and the resulting value is divided by the number of largest similarity values found to normalize the score. Note that missing values should be ignored, and the data types should be floats.
"""

def convert_tag(tag):
    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}
    try:
        return tag_dict[tag[0]]
    except KeyError:
        return None

def doc_to_synsets(doc):
    tokens = nltk.word_tokenize(doc)
    tags = nltk.pos_tag(tokens)

    synsets = []
    for word_tag in tags:
        word = word_tag[0]
        converted_pos=convert_tag(word_tag[1])
        synset_list = wordnet.synsets(word, pos=converted_pos)
        if len(synset_list)>0:
            synsets.append(synset_list[0])
    return synsets


def similarity_score(s1, s2):
    similarities=[]
    synsets1 = s1
    synsets2 = s2
    for syns1 in synsets1:
        similarities_syn1=[]
        Total_number_word=0
        
        for syns2 in synsets2:
            Total_number_word=Total_number_word+1
            path_similarity = syns1.path_similarity(syns2)
            
            if(path_similarity): 
                similarities_syn1.append(path_similarity)
                
        if(len(similarities_syn1)>0):
            similarities.append(max(similarities_syn1))
    similarity=sum(similarities)/len(similarities)
    
    return similarity

synsets1 = doc_to_synsets('I like cats')
synsets2 = doc_to_synsets('I like dogs')

similarity_score(synsets1, synsets2)

def document_path_similarity(doc1, doc2):
    """Finds the symmetrical similarity between doc1 and doc2"""

    synsets1 = doc_to_synsets(doc1)
    synsets2 = doc_to_synsets(doc2)

    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2

document_path_similarity("i like you",'Fish are nvqjp friends.')

paraphrases = pd.read_csv('paraphrases.csv')
paraphrases.head()

def most_similar_docs():
        
    max_similarity = 0
    max_pair = None

    for index, row in paraphrases.iterrows():
        similarity = document_path_similarity(row['D1'], row['D2'])
        if similarity > max_similarity:
            max_similarity = similarity
            max_pair = (row['D1'], row['D2'], similarity)

    return max_pair

most_similar_docs()

def label_accuracy():

    labels = []
    for index, row in paraphrases.iterrows():
        similarity = document_path_similarity(row['D1'], row['D2'])
        if similarity > 0.75:
            labels.append(1)  # Paraphrase
        else:
            labels.append(0)  # Not Paraphrase


    ground_truth_labels = paraphrases['Quality']

    accuracy = accuracy_score(ground_truth_labels, labels)
    return accuracy

label_accuracy()

